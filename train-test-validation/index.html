<!DOCTYPE html><html><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Train,Test, and Validation Sets</title><meta name="description" content="Double Descent"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="mlu_robot.5a492771.png"><link rel="stylesheet" href="main.f3a32faf.css"></head><body> <main></main>  <header> <nav> <span class="cardSpan" onclick="location.href='https://mlu-explain.github.io/';"> <span class="icon"> <object data="robot.cb528be1.svg" type="image/svg+xml"></object> </span> <span class="text"><h2>MLU-EXPL<span id="ai">AI</span>N</h2></span> </span> <ul id="toc"> <li> <a data-page="intro" href="#intro">Introduction</a> </li> <li> <a data-page="split" href="#split">The Split</a> </li> <li> <a data-page="train" href="#train">Train Set</a> </li> <li> <a data-page="model" href="#model">Model</a> </li> <li> <a data-page="validation" href="#validation">Validation Set</a> </li> <li> <a data-page="test" href="#test">Test Set</a> </li> <li> <a data-page="all" href="#all">Summary</a> </li> <div class="bubble"></div> </ul> </nav> </header>  <div id="scrolly"> <section data-index="0" class="intro-mobile" id="intro-mobile"> <h2>Train, Test, and Validation Sets <br></h2> <h4> By <a href="https://twitter.com/jdwlbr">Jared Wilber</a>   </h4> <p>  In most supervised machine learning tasks, best practice recommends to split your data into three independent sets: a <span id="annotation1"><b>training set</b></span>, a <span id="annotation2"><b>testing set</b></span>, and a <span id="annotation3"><b>validation set</b></span>. <br><br> To demo the reasons for splitting data in this manner, we will pretend that we have a dataset made of pets of the following two types: <br><br> <span class="center"><span class="b">Cats</span>:&nbsp; <img width="1.5rem" height="auto" class="inline-svg" src="noun_Cat_18061.575745a8.svg">&nbsp;&nbsp;&nbsp; <span class="b">Dogs</span>:&nbsp; <img width="1.5rem" height="auto" class="inline-svg" src="noun_Dog_79225.bf4c88d0.svg"></span> <br><br> For each pet in the dataset we know only two features: <span class="b">weight</span> and <span class="b">fluffiness</span>. <br><br> Our goal is to make use of the different data splits and identify a best model for classifying a given pet as either a cat or a dog, based on the available features. </p> </section> <figure> <div id="main-wrapper"> <div class="button-container"> <p>Select feature:</p> <button class="button" value="neither">None</button> <button class="active button" value="weight">Weight</button> <button class="button" value="fluffiness">Fluffiness</button> <button class="button" value="both">Both</button> </div> <div id="chart-wrapper"> <div id="chart"></div> <div id="table"></div> </div> </div>  </figure> <article> <section data-index="0" class="intro" id="intro"> <h2>The Importance of Data Splitting <br></h2> <p> By <a href="https://twitter.com/jdwlbr">Jared Wilber</a> & <a href="https://phonetool.amazon.com/users/bwernes">Brent Werness</a>.<br><br><br><br> In most supervised machine learning tasks, best practice recommends to split your data into three independent sets: a <span id="annotation1-intro"><b>training set</b></span>, a <span id="annotation2-intro"><b>testing set</b></span>, and a <span id="annotation3-intro"><b>validation set</b></span>. <br><br> To see why, let's pretend that we have a dataset of two types of pets: <br><br> <span class="center"><span class="b">Cats</span>:&nbsp; <img width="1.5rem" height="auto" class="inline-svg" src="noun_Cat_18061.575745a8.svg">&nbsp;&nbsp;&nbsp;&nbsp; <span class="b">Dogs</span>:&nbsp; <img width="1.5rem" height="auto" class="inline-svg" src="noun_Dog_79225.bf4c88d0.svg"></span> <br><br> For each pet in our dataset, we have two features: <span class="bold">weight</span> and <span class="bold">fluffiness</span>. <br><br> Our goal is to identify and evaluate suitable models for classifying a given pet as either a cat or a dog. We'll use train/test/validations splits to do this! </p> </section> <section data-index="1" class="split" id="split"> <h2>Train, Test, and Validation Splits</h2> <p> The first step in our classification task is to randomly  split our pets into three independent sets: <br><br> <span id="annotation4"><span class="bold">Training Set</span></span>: <span class="item-text">The dataset that we feed our model to learn potential underlying patterns and relationships.</span><br><br> <span id="annotation5"><span class="bold">Validation Set</span></span>: <span class="item-text">The dataset that we use to understand our model's performance across different model types and parameter choices.</span><br><br> <span id="annotation6"><span class="bold">Test Set</span></span>: <span class="item-text">The dataset that we use to approximate our model's accuracy in the wild.</span> </p> </section> <section data-index="2" class="train" id="train"> <h2>The Training Set</h2> <p> <span id="annotation7">The training set is the dataset that we employ to train our model.</span> It is this dataset that our model uses to learn every potential underlying pattern or relationship that will enable making predictions later on. <br><br> Since our model learns from it, it is very important that the training set be as representative as possible  of the population that we are trying to model. <br><br> Additionally, we need to be careful and ensure that it is as unbiased as possible, as any bias at this stage will be propagated downstream   </p> </section> <section data-index="3" class="model" id="model"> <h2>Building Our Model</h2> <p> Our goal is to determine whether a given pet is a cat or a dog. This is a binary classification task, so we will use a simple but effective model appropriate for this task: <a href="https://www.youtube.com/watch?v=jhJwNpidiqM"><span class="bold">logistic regression</span></a>. <br><br> Given a particular combination of the available features (<span class="bold">None</span>, <span class="bold">Weight</span>, <span class="bold">Fluffiness</span>, or <i>both</i> <span class="bold">Weight</span> and <span class="bold">Fluffiness</span>), the logistic regression classifier will generate a decision boundary to partition the pets  > </p> </section> <section data-index="4" class="validation" id="validation"> <h2>The Validation Set</h2> <p> For logistic regression, we can build four different classifiers &mdash; one for each choice of features to be included in the model: none  , just weight, just fluffiness, or both weight and fluffiness. <br><br> <b>How should we decide which model to select?</b> <br><br> We could compare the accuracy of each model on the training set, but doing so will result in a biased outcome &mdash; if we use the same exact dataset for both training and tuning, the model will overfit and will not generalize well beyond that dataset.<br><br> <b>This is where the validation set comes in</b> &mdash;  <b>Drag the feature across the line to see how the performance updates!</b> </p> </section> <section data-index="5" class="test" id="test"> <h2>The Testing Set</h2> <p> So, assuming you did not go too crazy moving the pets around, our best model (according to the validation set) is the model that takes both features into account. <br><br> Once we have used the validation set to determine the algorithm and parameter choices that we would like to use in production, the test set is used to approximate the models's true performance in the wild. <br><br>Let's repeat that again: <span id="annotation9">the test set is used as the final step in evaluating our model's performance on unseen data.</span> <br><br> <b>We should never, under any circumstance, look at the test set's performance before selecting a model.</b>  </p> </section> <section data-index="6" class="summary" id="summary"> <h2>Summary</h2> <p> You may have noticed that the test accuracy of the "just fluffiness" model was higher than that of the "both features" model, despite the validation set selecting the latter model as the best. This occurrence of the validation performance not exactly matching the test performance might happen, yet it is not a bad thing. Remember that the test performance is not a number to optimize over &mdash; it is a metric to assess future performance. It allows us to estimate, with confidence, that our model can distinguish between cats and dogs with 87.5% accuracy. <br><br> <span class="bold">Key Takeaways</span> <br><br> It is best practice in machine learning to split our data into the following three groups: <br><br> <span id="annotation10"><span class="bold">Training Set</span>: <span class="item-text">For training of the model.</span></span> <br><br> <span id="annotation11"> <span class="bold">Validation Set</span>: <span class="item-text">For unbiased evaluation of the model.</span></span><br><br> <span id="annotation12"><span class="bold">Test Set</span>: <span class="item-text">For final evaluation of the model.</span></span> <br><br> Adhering to this setup helps ensure that we have a realistic understanding of our model's performance, and that we (hopefully) built a model that generalizes well to unseen data. <br><br> <span class="center">üêæ</span> <br><br> Thanks for reading! To learn more about machine learning, check out <a href="https://aws.amazon.com/machine-learning/mlu/">our website</a>, watch <a href="https://www.youtube.com/channel/UC12LqyqTQYbXatYS9AA7Nuw">our videos</a>, or read the <a href="https://d2l.ai/"> D2L book</a>. <br><br> </p> </section> <section data-index="7" class="empty" id="empty"></section> <p id="end-p"></p> </article> </div> <section data-index="6" class="summary-mobile" id="summary-mobile"> <h2>Summary</h2> <p> You may have noticed that the test accuracy of the "just fluffiness" model was higher than that of the "both features" model, despite the validation set selecting the latter model as the best. This occurrence of the validation performance not exactly matching the test performance might happen, yet it is not a bad thing. Remember that the test performance is not a number to optimize over &mdash; it is a metric to assess future performance. It allows us to estimate, with confidence, that our model can distinguish between cats and dogs with 87.5% accuracy. <br><br> <span class="bold">Key Takeaways</span> <br><br> It is best practice in machine learning to split our data into the following three groups: <br><br> <span id="annotation10"><span class="bold">Training Set</span>: <span class="item-text">For training of the model.</span></span> <br><br> <span id="annotation11"> <span class="bold">Validation Set</span>: <span class="item-text">For unbiased evaluation of the model.</span></span><br><br> <span id="annotation12"><span class="bold">Test Set</span>: <span class="item-text">For final evaluation of the model.</span></span> <br><br> Adhering to this setup helps ensure that we have a realistic understanding of our model's performance, and that we (hopefully) built a model that generalizes well to unseen data. <br><br> <span class="center">üêæ</span> <br><br> Thanks for reading! To learn more about machine learning, check out <a href="https://aws.amazon.com/machine-learning/mlu/">our website</a>, watch <a href="https://www.youtube.com/channel/UC12LqyqTQYbXatYS9AA7Nuw">our videos</a>, or read the <a href="https://d2l.ai/"> D2L book</a>. <br><br> </p> </section> <script src="js.b70e5777.js"></script> <script src="roughAnnotations.f84e1103.js"></script> </body></html>