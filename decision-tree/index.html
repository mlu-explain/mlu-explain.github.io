<!DOCTYPE html><html><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Decision Trees</title><meta name="description" content="Decision Trees"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet" href="main.8910ea2f.css"></head><body> <main></main> <div id="scrolly"> <article> <section data-index="-1" id="title-section"> <section id="title"> <div id="intro-icon"> <a href="https://mla.corp.amazon.com/explain"><svg width="50" height="50" viewBox="0 0 234 216" fill="none" xmlns="https://www.w3.org/2000/svg"> <g id="mlu_robot 1" clip-path="url(#clip0)"> <g> <path id="Vector" d="M90.6641 83.1836C96.8828 83.1836 101.941 78.1289 101.941 71.8906V71.8242C101.941 65.5898 96.8945 60.5312 90.6641 60.5312C84.4453 60.5312 79.3828 65.5898 79.3828 71.8242V71.8906C79.3828 78.1289 84.4336 83.1836 90.6641 83.1836Z" fill="white"></path> <path id="Vector_2" d="M143.305 83.1836C149.523 83.1836 154.586 78.1289 154.586 71.8906V71.8242C154.586 65.5898 149.535 60.5312 143.305 60.5312C137.09 60.5312 132.027 65.5898 132.027 71.8242V71.8906C132.027 78.1289 137.078 83.1836 143.305 83.1836Z" fill="white"></path> <path id="Vector_3" d="M163.586 159.402H173.609V122.641H163.586V159.402Z" fill="white"></path> <path id="Vector_4" d="M60.3594 159.402H70.3867V122.641H60.3594V159.402Z" fill="white"></path> <g id="Group"> <path id="Vector_5" d="M182.16 30.0781H51.8047V10.0234H182.16V30.0781ZM182.16 103.609H51.8047V40.1055H182.16V103.609ZM144.559 168.789H89.4062V113.641H144.559V168.789ZM0 0V10.0234H15.8789V46.7891H25.9023V10.0234H41.7812V113.641H79.3867V178.816H96.9297V215.578H106.957V178.816H127.016V215.578H137.039V178.816H154.586V113.641H192.188V10.0234H233.969V0" fill="white"></path> </g> </g> </g> <defs> <clipPath id="clip0"> <rect width="233.97" height="215.58" fill="white"></rect> </clipPath> </defs> </svg> </a> <h2 class="logo">MLU-expl<span id="ai">AI</span>n</h2> </div> <h1>Decision Trees<br></h1> <p id="subtitle">The unreasonable power of nested decision rules.</p> <p> By <a href="https://phonetool.amazon.com/users/lucsan">Lucía Santamaría</a> & <a href="https://phonetool.amazon.com/users/wilberjw">Jared Wilber</a> <br><br><br><br> </p> </section> </section> <section data-index="0" id="intro"> <h2>How to Build a Decision Tree</h2> <p> Imagine that you were shown these data points and were asked to draw a series of horizontal and vertical lines to catch most of the apple trees as cleanly as possible. </p> </section> <section data-index="2" id="startsplit"> <h2>Start Splitting</h2> <p> Perhaps you will first come up with a vertical line just to the left of the area that contains most oak trees. </p> </section> <section data-index="3" id="moresplit"> <h2>Split Some More</h2> <p> Should you want to continue splitting in the most favourable manner, a subsequent horizontal line that left most of the apples trees above and the cherry trees below it does seem reasonable. </p> </section> <section data-index="4" id="moremoresplit"> <h2>And Some More</h2> <p> After this second split you are left with an area containing many apples and some cherries and oaks. No problem: a vertical division can be drawn to separate the classes still better. </p> </section> <section data-index="5" id="moremoremoresplit"> <h2>And Yet Some More</h2> <p> The remaining region just needs a further horizontal split and our job is almost done! </p> </section> <section data-index="6" id="best"> <h2>An Optimal Set of Nested Decisions</h2> <p> At this point you you can be quite satisfied with your good job separating the three classes. Yet you can’t help noticing that some regions still enclose a few data points of the wrong class. <br><br> Actually, nothing stops you from drawing further vertical and horizontal lines to create partitions as free from misclassifications as possible. <br><br> What would happen if you insist on further subdividing the space with the purpose of getting rid of any mislabeled data points? </p> </section> <section data-index="7" id="variance"> <h2>Don't Go Too Deep. The Danger of Overfitting</h2> <p> If you continued partitioning the space, the resulting regions would start becoming increasingly arbitrary. The Decision Tree would learn too much from the noise of these particular training examples and not enough generalizable rules. The classification power of such an overgrown tree would be anything but spectacular on unseen data.<br><br> Does this ring familiar? It is the well known trade-off between under and overfitting that we have explored in the <a href="https://mla.corp.amazon.com/explain/bias-variance">MLU bias-variance explainer</a>! <br><br> We will talk more about this phenomenom later. </p> </section> <section data-index="8" id="summary"> <h2>Summary</h2> <p>We just saw how a series of subsequent splits on the features of our training examples induce subregions of the space that can be used to classify any further data points. This is the core of how a Decision Tree works. <br><br> Next, scroll down to learn how we actually decide which features to split on at every node of the tree.</p> </section> </article> <div> <section id="intro-text"> <div id="intro-tree-chart"></div> <p> Decision trees are widely used algorithms for <span class="bold">supervised</span> machine learning. They can successfully handle both regression and classification problems. </p> <p> A Decision Tree consists of a <span class="bold">series of sequential decisions</span> on the data features that lead down a certain route to reach a specific answer for the label. The flow-like tree structure is navigated via conditional control statemets, or <span class="bold">if-then</span> rules, which split each decision node into two or more sub-nodes. </p> <p> To <span class="bold">train</span> a Decision Tree from data means to figure out the order in which the decisions should be assembled from the root to the leaves, as well as the values or conditions that need to be imposed on each parent branch in order to progress to the children nodes on the next level. </p> </section> <figure> <div id="chart-wrapper"> <div id="chart"></div> <div id="chart2"></div> </div> </figure> </div> </div> <section id="splits"> <h2>How Splits Are Determined:<br>The Goal is to Gain Information</h2> <p> The heuristic to select the splitting criterion to partition the data in the most optimal manner is based on the <span class="bold">information gain</span>. Information quantifies how "surprising" or "unexpected" an outcome is. At every decision node we want to split our data to yield the largest reduction in surprise or, equivalently, the most information gain. This is achieved by separating the training examples on the attributes that produce the most homogeneous branches. </p> <h3 class="subheader">Entropy</h3> <p> In information theory, the entropy is the quantity that measures the uncertainty or level of surprise of an event. The concept was proposed by <a href="https://en.wikipedia.org/wiki/Claude_Shannon">Claude Shannon</a> in 1948. If a sample is completely homogeneous, it is absolutely unsurprising and its entropy is zero. Conversely, an inhomogeneous sample has a large entropy value. The entropy can thus be used to quantify the <span class="bold">impurity</span> of a collection of labeled data points: a node containing multiple classes is impure whereas a node including only one class is pure. </p> <h3 class="subheader">It's Your Turn!</h3> <p> Below you can compute the entropy of a collection of labeled data points belonging to two classes, which is typical for <span class="bold">binary classification</span> problems. Click on the <span class="bold">Add</span> and <span class="bold">Remove</span> buttons to modify the composition of the bubble. Did you notice that pure samples have zero entropy whereas impure ones have larger entropy values? </p> <br> <div id="entropy-chart"></div> <h3 class="subheader">Using Information Gain to Build Decision Trees</h3> <p> With the intuition gained with the above animation, we can now describe the logic to train Decision Trees. The core algorithm, called <a href="https://en.wikipedia.org/wiki/ID3_algorithm">ID3</a>, iterates through all attributes or features of the data set and calculates the information gain based on the decrease of entropy before and after each split. The attribute that leads to the smallest entropy, or largest information gain, is selected. The data set is split on it and the process is repeated iteratively on all non-leaf branches top-down from the tree root in a greedy manner. </p> <p> Recursion stops if after a split all elements in a child node are of the same type: a leaf node is then created and labelled with the class of the examples. Additional stopping conditions may be imposed, such as requiring a minimum number of samples per leaf to continue splitting, or the value of the information gain exceeding a given threshold. </p> <p> <span class="bold">A note on information/impurity measures:</span>. An alternative to the entropy for the construction of Decision Trees is the <a href="https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity">Gini index</a>. Trees trained using Information Entropy or Gini Impurity are comparable, and only in a few cases do the results differ considerably. In the case of imbalanced data sets, entropy might be more prudent. Yet Gini might be more performant as it does not make use of logarithms as Entropy does. </p> </section> <section id="pertubations"> <h2>The Problem of Pertubations</h2> <p> Without question, Decision Trees have the advantage of being simple and fast to train and interpret. They also require minimal data preprocessing and are able to deal with outliers with ease. Yet they suffer from a major limitation, and that is their instability compared with other predictors. They can be <span class="bold">extremely sensitive to small perturbations in the data</span>: a minor change in the training examples can result in a drastical change in the structure of the Decision Tree. </p> <p> Below you can check for yourself how small random Gaussian perturbations on just a 10% of the training examples originates a set of completely different perturbed trees. </p> <br> <button type="button">Shuffle Data</button> <br><br> <div id="pertubation-wrapper"> <div id="tree-0" class="pertubation-item"></div> <div id="scatter-0" class="pertubation-item"></div> <div id="tree-1" class="pertubation-item"></div> <div id="scatter-1" class="pertubation-item"></div> <div id="tree-2" class="pertubation-item"></div> <div id="scatter-2" class="pertubation-item"></div> <div id="tree-3" class="pertubation-item"></div> <div id="scatter-3" class="pertubation-item"></div> <div id="tree-4" class="pertubation-item"></div> <div id="scatter-4" class="pertubation-item"></div> <div id="tree-5" class="pertubation-item"></div> <div id="scatter-5" class="pertubation-item"></div> <div id="tree-6" class="pertubation-item"></div> <div id="scatter-6" class="pertubation-item"></div> <div id="tree-7" class="pertubation-item"></div> <div id="scatter-7" class="pertubation-item"></div> </div> <h3 class="subheader">Why is This a Problem?</h3> <p> Well, unstable algorithms like Decision Trees lead to estimators with high variance, meaning that the trained predictor fails to clearly distinguish between persistent and random patterns in the data, a problem known as <span class="bold">overfitting</span>. </p> <p> Perhaps ironically, one way to alleviate the instability induced by perturbations is to introduce an extra layer of randomness in the training process. In practice this means to learn <span class="bold">multiple models</span> from bootstrapped samples of the data. </p> <p> Keep on reading to learn more about these <span class="bold">ensemble models</span>. </p> </section> <section id="limitations"> <h2>The Need to Go Beyond Decision Trees</h2> <p> As mentioned, the algorithm that trains the Decision Tree will continue creating splits of the data until all leaves are 100% pure, building a very deep tree. We have just also seen that instabilities due to perturbations give rise to estimators with high variance. Both aspects of Decision Trees are problematic, as they lead to <a href="https://mla.corp.amazon.com/explain/bias-variance">overfitted models</a> that do not generalize well when exposed to new data. </p> <p> There are ways to prevent the excessive growth of trees by pruning them, for instance constraining the maximum depth they can reach, limiting the number of leaves that can be created, or setting a minimum size for the amount of items in each leaf and not allowing leaves with too few items in them. </p> <p> Another way to avoid the issue altogether is to create collections of Decision Trees, the combined predictions of which do not suffer so heavily from this issue.<br><br> Enter <a href="https://mla.corp.amazon.com/explain/random-forest">Random Forests</a>. </p> </section> <section id="final"> <p> Thanks for reading! To learn more about machine learning, check out our <a href="https://aws.amazon.com/machine-learning/mlu/">self-paced courses</a>, our <a href="https://www.youtube.com/channel/UC12LqyqTQYbXatYS9AA7Nuw">YouTube videos</a>, and the <a href="https://d2l.ai/">Dive into Deep Learning</a> textbook. <br><br> If you have any comments or ideas related to <a href="https://mla.corp.amazon.com/explain">MLU-Explain articles</a>, feel free to reach out directly to <a href="https://twitter.com/jdwlbr">Jared </a> or <a href="https://twitter.com/lusantala">Lucía</a>. <br><br> The code for this article is available <a href="https://code.amazon.com/packages/Mlu-explain_decision-tree/trees/mainline">here</a>. </p> </section> <script src="js.ba414fce.js"></script> <script src="annotatedTree.4d838eb9.js"></script> </body></html>